{"cells":[{"cell_type":"markdown","metadata":{"id":"FIgMYjSx06o3"},"source":["## Upload image:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":38},"id":"addf525e"},"outputs":[{"data":{"text/html":["\n","     \u003cinput type=\"file\" id=\"files-39d32f08-5984-4a20-a0e8-a1a3dcf0e061\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" /\u003e\n","     \u003coutput id=\"result-39d32f08-5984-4a20-a0e8-a1a3dcf0e061\"\u003e\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      \u003c/output\u003e\n","      \u003cscript\u003e// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) =\u003e {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable\u003c!Object\u003e} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) =\u003e {\n","    inputElement.addEventListener('change', (e) =\u003e {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) =\u003e {\n","    cancel.onclick = () =\u003e {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) =\u003e {\n","      const reader = new FileReader();\n","      reader.onload = (e) =\u003e {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position \u003c fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","\u003c/script\u003e "],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"ename":"TypeError","evalue":"'NoneType' object is not subscriptable","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-3394260080.py\u001b[0m in \u001b[0;36m\u003ccell line: 0\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----\u003e 6\u001b[0;31m \u001b[0muploaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0muploaded\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m(target_dir)\u001b[0m\n\u001b[1;32m     70\u001b[0m   \"\"\"\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 72\u001b[0;31m   \u001b[0muploaded_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_upload_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultiple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m   \u001b[0;31m# Mapping from original filename to filename as saved locally.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m   \u001b[0mlocal_filenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36m_upload_files\u001b[0;34m(multiple)\u001b[0m\n\u001b[1;32m    169\u001b[0m   \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_collections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 171\u001b[0;31m   \u001b[0;32mwhile\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'action'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'complete'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m     result = _output.eval_js(\n\u001b[1;32m    173\u001b[0m         'google.colab._files._uploadFilesContinue(\"{output_id}\")'.format(\n","\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"]}],"source":["from google.colab import files\n","import io\n","import os\n","from PIL import Image\n","\n","uploaded = files.upload()\n","\n","if uploaded:\n","    output_folder = \"input\"\n","    os.makedirs(output_folder, exist_ok=True)\n","    print(f\"Saving uploaded images to: /{output_folder}/\")\n","\n","    for filename, content in uploaded.items():\n","        # Load the image using PIL\n","        image = Image.open(io.BytesIO(content))\n","        # Save the image to the specified output folder\n","        save_path = os.path.join(output_folder, filename)\n","        image.save(save_path)\n","\n","    print(f\"Images saved to {save_path}\")\n","else:\n","    print(\"No files were uploaded.\")"]},{"cell_type":"markdown","metadata":{"id":"bYNEflH24e3Z"},"source":["## Inference for blood cells detection"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8633,"status":"ok","timestamp":1765290653220,"user":{"displayName":"Johan Ivan Vega Bello","userId":"16925259825108720080"},"user_tz":300},"id":"pYupgyMO6JA_","outputId":"eeca2b27-c92b-46d1-d747-8e1216b0045c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting ultralytics\n","  Downloading ultralytics-8.3.235-py3-none-any.whl.metadata (37 kB)\n","Requirement already satisfied: numpy\u003e=1.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.0.2)\n","Requirement already satisfied: matplotlib\u003e=3.3.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (3.10.0)\n","Requirement already satisfied: opencv-python\u003e=4.6.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (4.12.0.88)\n","Requirement already satisfied: pillow\u003e=7.1.2 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (11.3.0)\n","Requirement already satisfied: pyyaml\u003e=5.3.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (6.0.3)\n","Requirement already satisfied: requests\u003e=2.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.32.4)\n","Requirement already satisfied: scipy\u003e=1.4.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.16.3)\n","Requirement already satisfied: torch\u003e=1.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.9.0+cu126)\n","Requirement already satisfied: torchvision\u003e=0.9.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (0.24.0+cu126)\n","Requirement already satisfied: psutil\u003e=5.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (5.9.5)\n","Requirement already satisfied: polars\u003e=0.20.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.31.0)\n","Collecting ultralytics-thop\u003e=2.0.18 (from ultralytics)\n","  Downloading ultralytics_thop-2.0.18-py3-none-any.whl.metadata (14 kB)\n","Requirement already satisfied: contourpy\u003e=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib\u003e=3.3.0-\u003eultralytics) (1.3.3)\n","Requirement already satisfied: cycler\u003e=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib\u003e=3.3.0-\u003eultralytics) (0.12.1)\n","Requirement already satisfied: fonttools\u003e=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib\u003e=3.3.0-\u003eultralytics) (4.61.0)\n","Requirement already satisfied: kiwisolver\u003e=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib\u003e=3.3.0-\u003eultralytics) (1.4.9)\n","Requirement already satisfied: packaging\u003e=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib\u003e=3.3.0-\u003eultralytics) (25.0)\n","Requirement already satisfied: pyparsing\u003e=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib\u003e=3.3.0-\u003eultralytics) (3.2.5)\n","Requirement already satisfied: python-dateutil\u003e=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib\u003e=3.3.0-\u003eultralytics) (2.9.0.post0)\n","Requirement already satisfied: charset_normalizer\u003c4,\u003e=2 in /usr/local/lib/python3.12/dist-packages (from requests\u003e=2.23.0-\u003eultralytics) (3.4.4)\n","Requirement already satisfied: idna\u003c4,\u003e=2.5 in /usr/local/lib/python3.12/dist-packages (from requests\u003e=2.23.0-\u003eultralytics) (3.11)\n","Requirement already satisfied: urllib3\u003c3,\u003e=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests\u003e=2.23.0-\u003eultralytics) (2.5.0)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests\u003e=2.23.0-\u003eultralytics) (2025.11.12)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch\u003e=1.8.0-\u003eultralytics) (3.20.0)\n","Requirement already satisfied: typing-extensions\u003e=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch\u003e=1.8.0-\u003eultralytics) (4.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch\u003e=1.8.0-\u003eultralytics) (75.2.0)\n","Requirement already satisfied: sympy\u003e=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch\u003e=1.8.0-\u003eultralytics) (1.14.0)\n","Requirement already satisfied: networkx\u003e=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch\u003e=1.8.0-\u003eultralytics) (3.6)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch\u003e=1.8.0-\u003eultralytics) (3.1.6)\n","Requirement already satisfied: fsspec\u003e=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch\u003e=1.8.0-\u003eultralytics) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch\u003e=1.8.0-\u003eultralytics) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch\u003e=1.8.0-\u003eultralytics) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch\u003e=1.8.0-\u003eultralytics) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch\u003e=1.8.0-\u003eultralytics) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch\u003e=1.8.0-\u003eultralytics) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch\u003e=1.8.0-\u003eultralytics) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch\u003e=1.8.0-\u003eultralytics) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch\u003e=1.8.0-\u003eultralytics) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch\u003e=1.8.0-\u003eultralytics) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch\u003e=1.8.0-\u003eultralytics) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch\u003e=1.8.0-\u003eultralytics) (2.27.5)\n","Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch\u003e=1.8.0-\u003eultralytics) (3.3.20)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch\u003e=1.8.0-\u003eultralytics) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch\u003e=1.8.0-\u003eultralytics) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch\u003e=1.8.0-\u003eultralytics) (1.11.1.6)\n","Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch\u003e=1.8.0-\u003eultralytics) (3.5.0)\n","Requirement already satisfied: six\u003e=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil\u003e=2.7-\u003ematplotlib\u003e=3.3.0-\u003eultralytics) (1.17.0)\n","Requirement already satisfied: mpmath\u003c1.4,\u003e=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy\u003e=1.13.3-\u003etorch\u003e=1.8.0-\u003eultralytics) (1.3.0)\n","Requirement already satisfied: MarkupSafe\u003e=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2-\u003etorch\u003e=1.8.0-\u003eultralytics) (3.0.3)\n","Downloading ultralytics-8.3.235-py3-none-any.whl (1.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ultralytics_thop-2.0.18-py3-none-any.whl (28 kB)\n","Installing collected packages: ultralytics-thop, ultralytics\n","Successfully installed ultralytics-8.3.235 ultralytics-thop-2.0.18\n"]}],"source":["!pip install ultralytics"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9446,"status":"ok","timestamp":1765290667957,"user":{"displayName":"Johan Ivan Vega Bello","userId":"16925259825108720080"},"user_tz":300},"id":"mCh9oy7-4JlV","outputId":"6cc96fc5-d72d-41b8-9905-156a5801eb6a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Creating new Ultralytics Settings v0.0.6 file ✅ \n","View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n","Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"]}],"source":["from ultralytics import YOLO\n","model = YOLO('/content/best_part_1.pt')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2688,"status":"ok","timestamp":1765290671886,"user":{"displayName":"Johan Ivan Vega Bello","userId":"16925259825108720080"},"user_tz":300},"id":"717423d1","outputId":"99d4fad8-3ed0-4022-b301-bf742a0c63fa"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","image 1/12 /content/input/BloodImage_00004.jpg: 480x640 18 RBCs, 1 WBC, 359.7ms\n","image 2/12 /content/input/BloodImage_00005.jpg: 480x640 24 RBCs, 1 WBC, 3 Plateletss, 165.9ms\n","image 3/12 /content/input/BloodImage_00006.jpg: 480x640 21 RBCs, 1 WBC, 2 Plateletss, 155.0ms\n","image 4/12 /content/input/BloodImage_00007.jpg: 480x640 26 RBCs, 1 WBC, 1 Platelets, 194.9ms\n","image 5/12 /content/input/BloodImage_00008.jpg: 480x640 23 RBCs, 1 WBC, 151.5ms\n","image 6/12 /content/input/BloodImage_00009.jpg: 480x640 24 RBCs, 1 WBC, 2 Plateletss, 172.8ms\n","image 7/12 /content/input/BloodImage_00010.jpg: 480x640 20 RBCs, 2 WBCs, 173.7ms\n","image 8/12 /content/input/BloodImage_00011.jpg: 480x640 27 RBCs, 1 WBC, 1 Platelets, 161.0ms\n","image 9/12 /content/input/BloodImage_00012.jpg: 480x640 25 RBCs, 1 WBC, 2 Plateletss, 173.0ms\n","image 10/12 /content/input/BloodImage_00013.jpg: 480x640 26 RBCs, 1 WBC, 191.2ms\n","image 11/12 /content/input/BloodImage_00014.jpg: 480x640 23 RBCs, 1 WBC, 2 Plateletss, 161.5ms\n","image 12/12 /content/input/BloodImage_00015.jpg: 480x640 18 RBCs, 1 WBC, 2 Plateletss, 171.4ms\n","Speed: 3.0ms preprocess, 186.0ms inference, 7.9ms postprocess per image at shape (1, 3, 480, 640)\n","Results saved to \u001b[1m/content/runs/detect/predict\u001b[0m\n","\n","Inference complete!\n"]}],"source":["import os\n","\n","input_images_folder = \"./input\"\n","results = model.predict(source=input_images_folder, save=True, conf=0.25, iou=0.7)\n","print(\"\\nInference complete!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24,"status":"ok","timestamp":1765290674257,"user":{"displayName":"Johan Ivan Vega Bello","userId":"16925259825108720080"},"user_tz":300},"id":"153b2455","outputId":"861653aa-4ace-434e-9ca3-8eb59e03a798"},"outputs":[{"name":"stdout","output_type":"stream","text":["Generating YOLO label files in: /content/output_part1/labeles\n","Preparing to save labeled images in: /content/output_part1/labeled_images\n","Found latest YOLO prediction output at: runs/detect/predict\n","Generated label for BloodImage_00004.jpg with 19 detections.\n","Copied labeled image BloodImage_00004.jpg to /content/output_part1/labeled_images/\n","Generated label for BloodImage_00005.jpg with 28 detections.\n","Copied labeled image BloodImage_00005.jpg to /content/output_part1/labeled_images/\n","Generated label for BloodImage_00006.jpg with 24 detections.\n","Copied labeled image BloodImage_00006.jpg to /content/output_part1/labeled_images/\n","Generated label for BloodImage_00007.jpg with 28 detections.\n","Copied labeled image BloodImage_00007.jpg to /content/output_part1/labeled_images/\n","Generated label for BloodImage_00008.jpg with 24 detections.\n","Copied labeled image BloodImage_00008.jpg to /content/output_part1/labeled_images/\n","Generated label for BloodImage_00009.jpg with 27 detections.\n","Copied labeled image BloodImage_00009.jpg to /content/output_part1/labeled_images/\n","Generated label for BloodImage_00010.jpg with 22 detections.\n","Copied labeled image BloodImage_00010.jpg to /content/output_part1/labeled_images/\n","Generated label for BloodImage_00011.jpg with 29 detections.\n","Copied labeled image BloodImage_00011.jpg to /content/output_part1/labeled_images/\n","Generated label for BloodImage_00012.jpg with 28 detections.\n","Copied labeled image BloodImage_00012.jpg to /content/output_part1/labeled_images/\n","Generated label for BloodImage_00013.jpg with 27 detections.\n","Copied labeled image BloodImage_00013.jpg to /content/output_part1/labeled_images/\n","Generated label for BloodImage_00014.jpg with 26 detections.\n","Copied labeled image BloodImage_00014.jpg to /content/output_part1/labeled_images/\n","Generated label for BloodImage_00015.jpg with 21 detections.\n","Copied labeled image BloodImage_00015.jpg to /content/output_part1/labeled_images/\n","\n","All YOLO label files and labeled images generated/copied successfully!\n"]}],"source":["import os\n","import glob\n","import shutil\n","\n","# Define the output directory for the generated YOLO label files\n","output_labels_dir = '/content/output_part1/labeles'\n","os.makedirs(output_labels_dir, exist_ok=True)\n","\n","# Define the output directory for the labeled images\n","output_labeled_images_dir = '/content/output_part1/labeled_images'\n","os.makedirs(output_labeled_images_dir, exist_ok=True)\n","\n","print(f\"Generating YOLO label files in: {output_labels_dir}\")\n","print(f\"Preparing to save labeled images in: {output_labeled_images_dir}\")\n","\n","# The 'results' variable holds the output from the last model.predict() call\n","if 'results' not in locals():\n","    print(\"Error: 'results' variable not found. Please run the model.predict() cell first.\")\n","else:\n","    # Find the most recent prediction run directory for the labeled images\n","    yolo_default_output_base_dir = 'runs/detect'\n","    list_of_runs = glob.glob(os.path.join(yolo_default_output_base_dir, 'predict*'))\n","    latest_run_dir = None\n","    if list_of_runs:\n","        latest_run_dir = max(list_of_runs, key=os.path.getctime)\n","        print(f\"Found latest YOLO prediction output at: {latest_run_dir}\")\n","    else:\n","        print(f\"Warning: No YOLO prediction run directories found in {yolo_default_output_base_dir}. Labeled images might not be available.\")\n","\n","    for result in results:\n","        # Get the original image filename\n","        image_filename = os.path.basename(result.path)\n","        base_name = os.path.splitext(image_filename)[0]\n","\n","        # --- Generate and save YOLO label file ---\n","        label_file_path = os.path.join(output_labels_dir, f\"{base_name}.txt\")\n","\n","        yolo_lines = []\n","        if result.boxes is not None:\n","            for box in result.boxes:\n","                class_id = int(box.cls[0])\n","                x_center_norm, y_center_norm, width_norm, height_norm = box.xywhn[0].tolist()\n","                yolo_lines.append(f\"{class_id} {x_center_norm:.6f} {y_center_norm:.6f} {width_norm:.6f} {height_norm:.6f}\")\n","\n","        with open(label_file_path, 'w') as f:\n","            f.write('\\n'.join(yolo_lines))\n","        print(f\"Generated label for {image_filename} with {len(yolo_lines)} detections.\")\n","\n","        # --- Copy labeled image ---\n","        if latest_run_dir:\n","            # The labeled images have the same filename as the input images\n","            src_labeled_img_path = os.path.join(latest_run_dir, image_filename)\n","            dst_labeled_img_path = os.path.join(output_labeled_images_dir, image_filename)\n","            if os.path.exists(src_labeled_img_path):\n","                shutil.copy(src_labeled_img_path, dst_labeled_img_path)\n","                print(f\"Copied labeled image {image_filename} to {output_labeled_images_dir}/\")\n","            else:\n","                print(f\"Warning: Labeled image {image_filename} not found in {latest_run_dir}\")\n","\n","    print(\"\\nAll YOLO label files and labeled images generated/copied successfully!\")\n"]},{"cell_type":"markdown","metadata":{"id":"QQ-tPSCd7PP5"},"source":["## WBD Extraction:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iRe1OXgNMPND"},"outputs":[],"source":["import os\n","import cv2\n","from pathlib import Path\n","\n","\n","def extract_category_crops(images_dir, labels_dir, output_dir, target_category, class_names=None):\n","    # Create output directory\n","    os.makedirs(output_dir, exist_ok=True)\n","\n","    # Get all label files\n","    label_files = sorted(Path(labels_dir).glob(\"*.txt\"))\n","\n","    crop_count = 0\n","\n","    for label_file in label_files:\n","        # Get corresponding image file\n","        image_name = label_file.stem\n","\n","        # Try common image extensions\n","        image_path = None\n","        for ext in ['.jpg', '.jpeg', '.png', '.JPG', '.JPEG', '.PNG']:\n","            potential_path = Path(images_dir) / f\"{image_name}{ext}\"\n","            if potential_path.exists():\n","                image_path = potential_path\n","                break\n","\n","        if image_path is None:\n","            print(f\"Warning: Image not found for {image_name}\")\n","            continue\n","\n","        # Read image\n","        image = cv2.imread(str(image_path))\n","        if image is None:\n","            print(f\"Warning: Could not read image {image_path}\")\n","            continue\n","\n","        height, width = image.shape[:2]\n","\n","        # Read annotations\n","        with open(label_file, 'r') as f:\n","            lines = f.readlines()\n","\n","        # Process each annotation\n","        for idx, line in enumerate(lines):\n","            parts = line.strip().split()\n","            if len(parts) \u003c 5:\n","                continue\n","\n","            class_id = int(parts[0])\n","\n","            # Check if this is the target category\n","            if class_id == target_category:\n","                #class_id x_center y_center width height (normalized 0-1)\n","                x_center = float(parts[1])\n","                y_center = float(parts[2])\n","                bbox_width = float(parts[3])\n","                bbox_height = float(parts[4])\n","\n","                # Convert to pixel coordinates\n","                x_center_px = x_center * width\n","                y_center_px = y_center * height\n","                bbox_width_px = bbox_width * width\n","                bbox_height_px = bbox_height * height\n","\n","                # Calculate bounding box corners\n","                x1 = int(x_center_px - bbox_width_px / 2)\n","                y1 = int(y_center_px - bbox_height_px / 2)\n","                x2 = int(x_center_px + bbox_width_px / 2)\n","                y2 = int(y_center_px + bbox_height_px / 2)\n","\n","                # Ensure coordinates are within image bounds\n","                x1 = max(0, x1)\n","                y1 = max(0, y1)\n","                x2 = min(width, x2)\n","                y2 = min(height, y2)\n","\n","                # Crop image\n","                cropped = image[y1:y2, x1:x2]\n","\n","                if cropped.size == 0:\n","                    continue\n","\n","                # Generate output filename\n","                if class_names and target_category \u003c len(class_names):\n","                    class_name = class_names[target_category]\n","                else:\n","                    class_name = f\"class_{target_category}\"\n","\n","                output_filename = f\"{class_name}_{image_name}_{idx}.jpg\"\n","                output_path = os.path.join(output_dir, output_filename)\n","\n","                # Save cropped image\n","                cv2.imwrite(output_path, cropped)\n","                crop_count += 1"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":34,"status":"ok","timestamp":1765290684806,"user":{"displayName":"Johan Ivan Vega Bello","userId":"16925259825108720080"},"user_tz":300},"id":"sw95xgBFD349","outputId":"a0405a17-62f5-426b-f226-630263921896"},"outputs":[{"name":"stdout","output_type":"stream","text":["Warning: 'WBC' not found in model.names. Using default category ID: 1\n","\n","Extracting WBCs from './input' using labels from '/content/output_part1/labeles'...\n","\n","WBC crops saved to: /content/extracted_cells/WBC\n"]}],"source":["# Define the directories based on your request\n","images_dir = \"./input\"\n","labels_dir = \"/content/output_part1/labeles\" # Using the path from the previous output\n","output_dir = \"/content/extracted_cells/WBC\"\n","\n","class_names = model.names\n","\n","# Find the target category ID for 'WBC'\n","# This assumes 'WBC' is present in model.names\n","if 'WBC' in class_names:\n","    target_category_id = class_names.index('WBC')\n","    print(f\"Found 'WBC' as category ID: {target_category_id}\")\n","else:\n","    # Fallback to hardcoded ID if 'WBC' not found or class_names is not set up correctly\n","    target_category_id = 1\n","    print(f\"Warning: 'WBC' not found in model.names. Using default category ID: {target_category_id}\")\n","\n","print(f\"\\nExtracting WBCs from '{images_dir}' using labels from '{labels_dir}'...\")\n","\n","extract_category_crops(\n","    images_dir=images_dir,\n","    labels_dir=labels_dir,\n","    output_dir=output_dir,\n","    target_category=target_category_id,\n","    class_names=class_names\n",")\n","\n","print(f\"\\nWBC crops saved to: {output_dir}\")"]},{"cell_type":"markdown","metadata":{"id":"1PB2ikKYMpxJ"},"source":["## Image Downsamplin and letterbox (for classification model):"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":43,"status":"ok","timestamp":1765290687264,"user":{"displayName":"Johan Ivan Vega Bello","userId":"16925259825108720080"},"user_tz":300},"id":"btsfhWXnMmj3","outputId":"a9d28009-16fb-44cb-b510-5f48fd0c639d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Processed 13 images in 0.03s\n","Average time per image: 1.94ms\n"]}],"source":["import cv2\n","import numpy as np\n","import os\n","from pathlib import Path\n","import time\n","\n","def rescale_images_for_yolo(input_dir, output_dir, target_size=(84, 84),\n","                            interpolation=cv2.INTER_AREA):\n","\n","    os.makedirs(output_dir, exist_ok=True)\n","\n","    image_files = []\n","    for ext in ['*.jpg', '*.jpeg', '*.png', '*.JPG', '*.JPEG', '*.PNG']:\n","        image_files.extend(Path(input_dir).glob(ext))\n","\n","    total_time = 0\n","\n","    for img_path in image_files:\n","        start = time.time()\n","\n","        # Read image\n","        img = cv2.imread(str(img_path))\n","        if img is None:\n","            print(f\"Warning: Could not read {img_path}\")\n","            continue\n","\n","        resized = letterbox_resize(img, target_size, interpolation)\n","\n","        # Save\n","        output_path = os.path.join(output_dir, img_path.name)\n","        cv2.imwrite(output_path, resized)\n","\n","        total_time += time.time() - start\n","\n","    print(f\"Processed {len(image_files)} images in {total_time:.2f}s\")\n","    print(f\"Average time per image: {total_time/len(image_files)*1000:.2f}ms\")\n","\n","\n","def letterbox_resize(img, target_size, interpolation=cv2.INTER_AREA):\n","    target_w, target_h = target_size\n","    h, w = img.shape[:2]\n","\n","    # Calculate scaling factor\n","    scale = min(target_w / w, target_h / h)\n","    new_w = int(w * scale)\n","    new_h = int(h * scale)\n","\n","    # Resize image\n","    resized = cv2.resize(img, (new_w, new_h), interpolation=interpolation)\n","\n","    # Create canvas with padding\n","    canvas = np.full((target_h, target_w, 3), (202, 207, 206), dtype=np.uint8)\n","\n","    # Calculate padding\n","    pad_w = (target_w - new_w) // 2\n","    pad_h = (target_h - new_h) // 2\n","\n","    # Place resized image on canvas\n","    canvas[pad_h:pad_h + new_h, pad_w:pad_w + new_w] = resized\n","\n","    return canvas\n","\n","\n","rescaled_images = \"/content/extracted_cells/WBC\"\n","output_dir = \"/content/input_model2\"\n","\n","rescale_images_for_yolo(\n","        input_dir=rescaled_images,\n","        output_dir=output_dir,\n","        target_size=(84, 84),\n","        interpolation=cv2.INTER_AREA\n","    )"]},{"cell_type":"markdown","metadata":{"id":"U1RX8VYYTrTj"},"source":["## WBC Classification:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8011,"status":"ok","timestamp":1765290698421,"user":{"displayName":"Johan Ivan Vega Bello","userId":"16925259825108720080"},"user_tz":300},"id":"WOMegO-FTvB2","outputId":"1d52caad-7aa0-4c81-c608-9eb000980e80"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using device: cpu\n","Successfully loaded weights from /content/best_wbc_resnet50.pth\n","Found images in /content/input_model2 for classification.\n","\n","--- Classification Results ---\n","Image: WBC_BloodImage_00004_2.jpg -\u003e Predicted Class: eosinophil (Confidence: 0.53)\n","Image: WBC_BloodImage_00005_5.jpg -\u003e Predicted Class: neutrophil (Confidence: 0.87)\n","Image: WBC_BloodImage_00006_2.jpg -\u003e Predicted Class: neutrophil (Confidence: 0.96)\n","Image: WBC_BloodImage_00007_2.jpg -\u003e Predicted Class: eosinophil (Confidence: 0.40)\n","Image: WBC_BloodImage_00008_1.jpg -\u003e Predicted Class: monocyte (Confidence: 0.73)\n","Image: WBC_BloodImage_00009_2.jpg -\u003e Predicted Class: eosinophil (Confidence: 0.98)\n","Image: WBC_BloodImage_00010_5.jpg -\u003e Predicted Class: monocyte (Confidence: 0.80)\n","Image: WBC_BloodImage_00010_7.jpg -\u003e Predicted Class: neutrophil (Confidence: 0.81)\n","Image: WBC_BloodImage_00011_1.jpg -\u003e Predicted Class: eosinophil (Confidence: 0.50)\n","Image: WBC_BloodImage_00012_1.jpg -\u003e Predicted Class: eosinophil (Confidence: 0.77)\n","Image: WBC_BloodImage_00013_4.jpg -\u003e Predicted Class: neutrophil (Confidence: 0.87)\n","Image: WBC_BloodImage_00014_4.jpg -\u003e Predicted Class: neutrophil (Confidence: 0.51)\n","Image: WBC_BloodImage_00015_1.jpg -\u003e Predicted Class: monocyte (Confidence: 0.95)\n","\n","Classification complete!\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torchvision.transforms as transforms\n","import torchvision.models as models\n","from PIL import Image\n","import os\n","import glob\n","\n","# 1. Device setup\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","# 2. Model Definition\n","num_classes = 5\n","\n","model = models.resnet50(weights=None)\n","# Modify the final classification layer to match the number of classes\n","num_ftrs = model.fc.in_features\n","model.fc = nn.Linear(num_ftrs, num_classes)\n","\n","# 3. Load Weights\n","weights_path = \"/content/best_wbc_resnet50.pth\"\n","if os.path.exists(weights_path):\n","    # Load state_dict onto the appropriate device\n","    model.load_state_dict(torch.load(weights_path, map_location=device))\n","    print(f\"Successfully loaded weights from {weights_path}\")\n","else:\n","    print(f\"Error: Weights file not found at {weights_path}. Please ensure it's in the /content/ folder.\")\n","    print(\"Cannot proceed with classification without model weights.\")\n","\n","model = model.to(device)\n","model.eval() # Set model to evaluation mode\n","\n","# 4. Image Transformations\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # ImageNet normalization\n","])\n","\n","# 5. Dataset and DataLoader\n","input_dir_for_classification = \"/content/input_model2\"\n","\n","# Check if the directory exists and contains images\n","if not os.path.exists(input_dir_for_classification) or not os.listdir(input_dir_for_classification):\n","    print(f\"Error: No images found in {input_dir_for_classification}. Please ensure previous steps ran correctly.\")\n","else:\n","    print(f\"Found images in {input_dir_for_classification} for classification.\")\n","\n","    # Create a list of image paths\n","    image_paths = []\n","    for ext in ['*.jpg', '*.jpeg', '*.png', '*.JPG', '*.JPEG', '*.PNG']:\n","        image_paths.extend(glob.glob(os.path.join(input_dir_for_classification, ext)))\n","    image_paths.sort() # Ensure consistent order\n","\n","    if not image_paths:\n","        print(f\"No image files found in {input_dir_for_classification} with common extensions. Nothing to classify.\")\n","    else:\n","        # Define class names based on the order your model was trained\n","        class_names = [\"basophil\", \"eosinophil\", \"lymphocyte\", \"monocyte\", \"neutrophil\"] # Updated class names\n","\n","        print(f\"\\n--- Classification Results ---\")\n","        with torch.no_grad(): # Disable gradient calculation for inference\n","            for img_path in image_paths:\n","                img_name = os.path.basename(img_path)\n","                try:\n","                    image = Image.open(img_path).convert(\"RGB\") # Ensure 3 channels for ResNet\n","                    input_tensor = transform(image)\n","                    input_batch = input_tensor.unsqueeze(0).to(device) # Add a batch dimension\n","\n","                    output = model(input_batch)\n","                    probabilities = torch.nn.functional.softmax(output, dim=1)\n","                    # Get the predicted class with the highest probability\n","                    _, predicted_class_idx = torch.max(probabilities, 1)\n","                    confidence = probabilities[0][predicted_class_idx].item()\n","\n","                    if predicted_class_idx.item() \u003c len(class_names):\n","                        predicted_class_name = class_names[predicted_class_idx.item()]\n","                    else:\n","                        predicted_class_name = f\"Class_{predicted_class_idx.item()}\" # Fallback if index out of bounds\n","\n","                    print(f\"Image: {img_name} -\u003e Predicted Class: {predicted_class_name} (Confidence: {confidence:.2f})\")\n","\n","                except Exception as e:\n","                    print(f\"Could not process image {img_name}: {e}\")\n","\n","        print(\"\\nClassification complete!\")\n"]},{"cell_type":"markdown","metadata":{"id":"bb319c71"},"source":["## Combine and Display Comprehensive Output"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1250,"status":"ok","timestamp":1765290704001,"user":{"displayName":"Johan Ivan Vega Bello","userId":"16925259825108720080"},"user_tz":300},"id":"b67bc6d8","outputId":"f2fc1fb8-a5c8-455b-9272-c3c8da1cda01"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using device: cpu\n","Successfully loaded weights from /content/best_wbc_resnet50.pth\n","Found images in /content/input_model2 for classification.\n","\n","--- Classification Results ---\n","Image: WBC_BloodImage_00004_2.jpg -\u003e Predicted Class: eosinophil (Confidence: 0.53)\n","Image: WBC_BloodImage_00005_5.jpg -\u003e Predicted Class: neutrophil (Confidence: 0.87)\n","Image: WBC_BloodImage_00006_2.jpg -\u003e Predicted Class: neutrophil (Confidence: 0.96)\n","Image: WBC_BloodImage_00007_2.jpg -\u003e Predicted Class: eosinophil (Confidence: 0.40)\n","Image: WBC_BloodImage_00008_1.jpg -\u003e Predicted Class: monocyte (Confidence: 0.73)\n","Image: WBC_BloodImage_00009_2.jpg -\u003e Predicted Class: eosinophil (Confidence: 0.98)\n","Image: WBC_BloodImage_00010_5.jpg -\u003e Predicted Class: monocyte (Confidence: 0.80)\n","Image: WBC_BloodImage_00010_7.jpg -\u003e Predicted Class: neutrophil (Confidence: 0.81)\n","Image: WBC_BloodImage_00011_1.jpg -\u003e Predicted Class: eosinophil (Confidence: 0.50)\n","Image: WBC_BloodImage_00012_1.jpg -\u003e Predicted Class: eosinophil (Confidence: 0.77)\n","Image: WBC_BloodImage_00013_4.jpg -\u003e Predicted Class: neutrophil (Confidence: 0.87)\n","Image: WBC_BloodImage_00014_4.jpg -\u003e Predicted Class: neutrophil (Confidence: 0.51)\n","Image: WBC_BloodImage_00015_1.jpg -\u003e Predicted Class: monocyte (Confidence: 0.95)\n","\n","Classification complete!\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torchvision.transforms as transforms\n","import torchvision.models as models\n","from PIL import Image\n","import os\n","import glob\n","\n","wbc_classification_results = [] # Initialize the list to store results\n","\n","# 1. Device setup\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","# 2. Model Definition\n","num_classes = 5\n","\n","model = models.resnet50(weights=None)\n","# Modify the final classification layer to match the number of classes\n","num_ftrs = model.fc.in_features\n","model.fc = nn.Linear(num_ftrs, num_classes)\n","\n","# 3. Load Weights\n","weights_path = \"/content/best_wbc_resnet50.pth\"\n","if os.path.exists(weights_path):\n","    # Load state_dict onto the appropriate device\n","    model.load_state_dict(torch.load(weights_path, map_location=device))\n","    print(f\"Successfully loaded weights from {weights_path}\")\n","else:\n","    print(f\"Error: Weights file not found at {weights_path}. Please ensure it's in the /content/ folder.\")\n","    print(\"Cannot proceed with classification without model weights.\")\n","\n","model = model.to(device)\n","model.eval() # Set model to evaluation mode\n","\n","# 4. Image Transformations\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # ImageNet normalization\n","])\n","\n","# 5. Dataset and DataLoader\n","input_dir_for_classification = \"/content/input_model2\"\n","\n","# Check if the directory exists and contains images\n","if not os.path.exists(input_dir_for_classification) or not os.listdir(input_dir_for_classification):\n","    print(f\"Error: No images found in {input_dir_for_classification}. Please ensure previous steps ran correctly.\")\n","else:\n","    print(f\"Found images in {input_dir_for_classification} for classification.\")\n","\n","    # Create a list of image paths\n","    image_paths = []\n","    for ext in ['*.jpg', '*.jpeg', '*.png', '*.JPG', '*.JPEG', '*.PNG']:\n","        image_paths.extend(glob.glob(os.path.join(input_dir_for_classification, ext)))\n","    image_paths.sort() # Ensure consistent order\n","\n","    if not image_paths:\n","        print(f\"No image files found in {input_dir_for_classification} with common extensions. Nothing to classify.\")\n","    else:\n","        # Define class names based on the order your model was trained\n","        class_names = [\"basophil\", \"eosinophil\", \"lymphocyte\", \"monocyte\", \"neutrophil\"]\n","\n","        print(f\"\\n--- Classification Results ---\")\n","        with torch.no_grad(): # Disable gradient calculation for inference\n","            for img_path in image_paths:\n","                img_name = os.path.basename(img_path)\n","                try:\n","                    image = Image.open(img_path).convert(\"RGB\") # Ensure 3 channels for ResNet\n","                    input_tensor = transform(image)\n","                    input_batch = input_tensor.unsqueeze(0).to(device) # Add a batch dimension\n","\n","                    output = model(input_batch)\n","                    probabilities = torch.nn.functional.softmax(output, dim=1)\n","                    # Get the predicted class with the highest probability\n","                    _, predicted_class_idx = torch.max(probabilities, 1)\n","                    confidence = probabilities[0][predicted_class_idx].item()\n","\n","                    if predicted_class_idx.item() \u003c len(class_names):\n","                        predicted_class_name = class_names[predicted_class_idx.item()]\n","                    else:\n","                        predicted_class_name = f\"Class_{predicted_class_idx.item()}\" # Fallback if index out of bounds\n","\n","                    # Extract original_image_source (e.g., 'WBC_BloodImage_00016_1.jpg' -\u003e 'BloodImage_00016.jpg')\n","                    parts = img_name.split('_')\n","                    # Rejoin all parts except the first ('WBC') and the last (index.jpg) then add '.jpg'\n","                    original_image_source = '_'.join(parts[1:-1]) + '.jpg'\n","\n","                    wbc_classification_results.append({\n","                        'crop_path': img_path,\n","                        'predicted_class': predicted_class_name,\n","                        'confidence': confidence,\n","                        'original_image_source': original_image_source\n","                    })\n","\n","                    print(f\"Image: {img_name} -\u003e Predicted Class: {predicted_class_name} (Confidence: {confidence:.2f})\")\n","\n","                except Exception as e:\n","                    print(f\"Could not process image {img_name}: {e}\")\n","\n","        print(\"\\nClassification complete!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"158qenEA8slU_mB11UkYJusz32ua8lVUc"},"executionInfo":{"elapsed":8400,"status":"ok","timestamp":1765290716959,"user":{"displayName":"Johan Ivan Vega Bello","userId":"16925259825108720080"},"user_tz":300},"id":"f295f03d","outputId":"3fb807f3-d564-4ba8-b2cc-a9f29183d88b"},"outputs":[{"data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{},"output_type":"display_data"}],"source":["import matplotlib.pyplot as plt\n","from PIL import Image, ImageDraw\n","import os\n","import glob\n","import pandas as pd\n","import numpy as np\n","import math\n","import io\n","\n","# --- 1. Define input and output directories and class names ---\n","input_images_dir = './input'\n","output_labeled_images_dir = '/content/output_part1/labeled_images'\n","labels_dir = '/content/output_part1/labeles'\n","\n","# Initialize class names for detection model (RBC, WBC, Platelets)\n","class_names_from_model = ['RBC', 'WBC', 'Platelets']\n","# Attempt to get them from 'model' if it's available in the kernel state\n","if 'model' in locals() and hasattr(model, 'names'):\n","    class_names_from_model = model.names\n","class_id_to_name = {i: name for i, name in enumerate(class_names_from_model)}\n","\n","# Initialize class names for WBC classification model\n","wbc_classification_class_names = [\"basophil\", \"eosinophil\", \"lymphocyte\", \"monocyte\", \"neutrophil\"] # Updated class names\n","\n","if 'class_names' in locals() and isinstance(class_names, list) and len(class_names) == 5:\n","    wbc_classification_class_names = class_names\n","\n","# --- 2. Ensure wbc_classification_results global list is available ---\n","# This list should now be populated from the previous step\n","if 'wbc_classification_results' not in locals() or not isinstance(wbc_classification_results, list):\n","    print(\"Warning: 'wbc_classification_results' not found or not a list. Initializing as empty.\")\n","    wbc_classification_results = []\n","\n","# --- Helper function to render DataFrame as an image ---\n","def dataframe_to_image(df, title=\"\", dpi=100):\n","    fig, ax = plt.subplots(figsize=(df.shape[1] * 1.5, df.shape[0] * 0.6), dpi=dpi)\n","    ax.axis('off')\n","    ax.axis('tight')\n","    # Create the table\n","    table = ax.table(cellText=df.values, colLabels=df.columns, loc='center', cellLoc='center')\n","    table.auto_set_font_size(False)\n","    table.set_fontsize(10)\n","    table.scale(1.2, 1.2)\n","    ax.set_title(title, fontsize=12, pad=10)\n","\n","    # Save to buffer and load as PIL Image\n","    buf = io.BytesIO()\n","    plt.savefig(buf, format='png', bbox_inches='tight', dpi=dpi)\n","    plt.close(fig)\n","    buf.seek(0)\n","    img = Image.open(buf)\n","    return img\n","\n","# --- Helper function to render WBC crops grid as an image ---\n","def wbc_crops_grid_to_image(wbc_crops_data, original_img_filename, dpi=100):\n","    if not wbc_crops_data:\n","        # Create a placeholder image if no WBCs found for this image\n","        dummy_img = Image.new('RGB', (300, 150), color = (200, 200, 200))\n","        draw = ImageDraw.Draw(dummy_img)\n","        text = f\"No WBCs found\\nfor {original_img_filename}\"\n","        # Simple text positioning - ideally dynamically calculate for centering\n","        draw.text((10, 50), text, fill=(0,0,0))\n","        return dummy_img\n","\n","    num_wbc_crops = len(wbc_crops_data)\n","    num_cols = 3\n","    num_rows = math.ceil(num_wbc_crops / num_cols)\n","\n","    # Calculate figure size dynamically\n","    fig_width = num_cols * 2.5 # Each crop about 2.5 inches wide\n","    fig_height = num_rows * 2.5 + 0.5 # Each crop about 2.5 inches high + some title space\n","    fig, axes = plt.subplots(num_rows, num_cols, figsize=(fig_width, fig_height), dpi=dpi)\n","\n","    if num_rows == 1 and num_cols == 1 and num_wbc_crops == 1: # Handle single subplot case\n","        axes = np.array([axes]) # Make it iterable like other cases\n","    else:\n","        axes = axes.flatten()\n","\n","    for i, wbc_result in enumerate(wbc_crops_data):\n","        crop_path = wbc_result['crop_path']\n","        predicted_class = wbc_result['predicted_class']\n","        confidence = wbc_result['confidence']\n","\n","        if i \u003c len(axes):\n","            ax = axes[i]\n","            try:\n","                crop_img = Image.open(crop_path)\n","                ax.imshow(crop_img)\n","                ax.set_title(f\"{predicted_class.capitalize()}\\n(Conf: {confidence:.2f})\", fontsize=8)\n","                ax.axis('off')\n","            except Exception as e:\n","                print(f\"Error displaying crop {os.path.basename(crop_path)}: {e}\")\n","                ax.text(0.5, 0.5, 'Error', ha='center', va='center', transform=ax.transAxes, color='red')\n","                ax.axis('off')\n","\n","    # Hide any unused subplots\n","    for j in range(i + 1, len(axes)):\n","        axes[j].axis('off')\n","\n","    plt.suptitle(f\"Classified WBCs from {original_img_filename}\", fontsize=14, y=0.99)\n","    plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # Adjust layout to prevent title overlap\n","\n","    # Save to buffer and load as PIL Image\n","    buf = io.BytesIO()\n","    plt.savefig(buf, format='png', bbox_inches='tight', dpi=dpi)\n","    plt.close(fig)\n","    buf.seek(0)\n","    img = Image.open(buf)\n","    return img\n","\n","# --- 3. Get a sorted list of all original image filenames ---\n","original_image_filenames = []\n","for ext in ['*.jpg', '*.jpeg', '*.png', '*.JPG', '*.JPEG', '*.PNG']:\n","    original_image_filenames.extend([os.path.basename(p) for p in glob.glob(os.path.join(input_images_dir, ext))])\n","original_image_filenames.sort()\n","\n","if not original_image_filenames:\n","    print(f\"No original images found in {input_images_dir} to process.\")\n","else:\n","    print(f\"Processing {len(original_image_filenames)} original images...\")\n","\n","    for original_img_filename in original_image_filenames:\n","        print(f\"\\n--- Processing {original_img_filename} ---\")\n","\n","        # a. Get the Labeled Image\n","        labeled_img_path = os.path.join(output_labeled_images_dir, original_img_filename)\n","        if os.path.exists(labeled_img_path):\n","            labeled_img_pil = Image.open(labeled_img_path).convert('RGB')\n","        else:\n","            print(f\"Warning: Labeled image not found for {original_img_filename}. Creating placeholder.\")\n","            labeled_img_pil = Image.new('RGB', (640, 480), color = (100, 100, 100))\n","            draw = ImageDraw.Draw(labeled_img_pil)\n","            draw.text((10, 10), f\"Labeled Image Missing:\\n{original_img_filename}\", fill=(255,255,255))\n","\n","        # b. Create a table image summarizing blood cell counts\n","        base_name = os.path.splitext(original_img_filename)[0]\n","        label_file_path = os.path.join(labels_dir, f\"{base_name}.txt\")\n","\n","        counts_data = {'Image': original_img_filename, 'RBC': 0, 'WBC': 0, 'Platelets': 0}\n","\n","        if os.path.exists(label_file_path):\n","            with open(label_file_path, 'r') as f:\n","                lines = f.readlines()\n","            for line in lines:\n","                parts = line.strip().split()\n","                if len(parts) \u003e 0:\n","                    class_id = int(parts[0])\n","                    class_name = class_id_to_name.get(class_id, f\"Unknown_{class_id}\")\n","                    if class_name in counts_data: # Only count known classes\n","                        counts_data[class_name] += 1\n","        else:\n","            print(f\"Warning: Label file not found for {original_img_filename}. Counts will be zero.\")\n","\n","        counts_df = pd.DataFrame([counts_data])\n","        table_img_pil = dataframe_to_image(counts_df, title=\"Cell Detection Counts\")\n","\n","        # c. Render a 3-column grid of its classified WBC crops\n","        wbc_crops_for_image = [\n","            res for res in wbc_classification_results\n","            if res['original_image_source'] == original_img_filename\n","        ]\n","        wbc_grid_img_pil = wbc_crops_grid_to_image(wbc_crops_for_image, original_img_filename)\n","\n","        # d. Combine these three visual elements into a single, cohesive image\n","\n","        # Resize labeled image to a reasonable width if too large, maintain aspect ratio\n","        target_labeled_width = 600\n","        if labeled_img_pil.width \u003e target_labeled_width:\n","            labeled_img_pil = labeled_img_pil.resize((target_labeled_width, int(labeled_img_pil.height * target_labeled_width / labeled_img_pil.width)), Image.Resampling.LANCZOS)\n","\n","        # Find the maximum width among the three for consistent display\n","        max_combine_width = max(labeled_img_pil.width, table_img_pil.width, wbc_grid_img_pil.width)\n","\n","        # Create new images with the common width, padding with white if narrower\n","        def pad_image_to_width(img, target_width, fill_color=(255,255,255)):\n","            if img.width \u003c target_width:\n","                new_img = Image.new('RGB', (target_width, img.height), fill_color)\n","                offset_x = (target_width - img.width) // 2\n","                new_img.paste(img, (offset_x, 0))\n","                return new_img\n","            return img\n","\n","        padded_labeled_img = pad_image_to_width(labeled_img_pil, max_combine_width)\n","        padded_table_img = pad_image_to_width(table_img_pil, max_combine_width)\n","        padded_wbc_grid_img = pad_image_to_width(wbc_grid_img_pil, max_combine_width)\n","\n","        final_height = padded_labeled_img.height + padded_table_img.height + padded_wbc_grid_img.height\n","        combined_image = Image.new('RGB', (max_combine_width, final_height), color=(255, 255, 255))\n","\n","        y_offset = 0\n","        combined_image.paste(padded_labeled_img, (0, y_offset))\n","        y_offset += padded_labeled_img.height\n","        combined_image.paste(padded_table_img, (0, y_offset))\n","        y_offset += padded_table_img.height\n","        combined_image.paste(padded_wbc_grid_img, (0, y_offset))\n","\n","        # Display the combined image\n","        plt.figure(figsize=(max_combine_width / 100, final_height / 100)) # Adjust figsize based on image px and dpi\n","        plt.imshow(combined_image)\n","        plt.title(f\"Comprehensive Analysis for: {original_img_filename}\")\n","        plt.axis('off')\n","        plt.show()\n","\n","print(\"\\nFinished generating and displaying comprehensive outputs for all images.\")"]}],"metadata":{"colab":{"name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}